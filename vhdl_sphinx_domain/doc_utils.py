""" Various Sphinx and docutils processing functions
"""

from sphinx.util.nodes import nested_parse_with_titles

from docutils import nodes
from docutils.statemachine import ViewList
from docutils.utils.code_analyzer import Lexer, LexerError
from docutils.utils import unescape

def parse_rest(state, lines, class_dict={}):
    """ Parse a list of lines into a list of docutil nodes.

    Nodes are not part of a top object so sections found in the parsed text can be mede part of the parent hierarchy.
    User-specified classes can be applied to objects specified in `class_dict`.

    Parameters:

        state:

        lines (list of str): text to be parsed

        class_dict (dict): Dict in the format {object_type:class_name,...} or {object_type:[class_name, class_name...],...} that applies the specified ``class_name`` to the objects generated by the REST parser that have the tag ``object_type``

    Returns:
        list: docutil nodes

    """
    if not lines: return []
    container_node = nodes.paragraph('','')
    # print(f'Parsing text as restructuredText')
    nested_parse_with_titles(state, ViewList(lines, source=''), container_node)
    for n in container_node:
        if n.tagname in class_dict:
            # print '*** Updating class on object:', n.tagname
            if isinstance(class_dict[n.tagname], str):
                n['classes'].append(class_dict[n.tagname])
            else:
                n['classes'].extend(class_dict[n.tagname])
    return list(container_node)


def parse_markdown_table(state, rows, pos):
    """ Attempts to decode a block of text as a markdown table

    Returns:

        tuple or None: tuple ``(new position, docutils_table_node)`` returned if a markdown table was found,
            and `None` is returned if there is no markdown table at this location.
    """

    # Define some helper functions first
    def split_row(rows, pos):
        if pos >= len(rows):
            return []
        e = [s.strip() for s in rows[pos].strip().split('|')]
        if len(e) < 2:
            return []
        if not e[0]:
            del e[0]
        if not e[-1]:
            del e[-1]
        return e

    def build_row(entries, alignment):
        # return nodes.row('', *[nodes.entry('', nodes.inline('', e)) for e in entries])
        node_list = []
        for i, e in enumerate(entries):
            n = parse_rest(state, [e])
            # print '********** Element node = ', n
            entry = nodes.entry('', *n)
            if i < len(alignment) and alignment[i]:
                entry['classes'].append(alignment[i])
            node_list.append(entry)
        return nodes.row('', *node_list)

    def max_column_widths(*rows):
        widths = [0] * max(len(r) for r in rows)
        for row in rows:
            for i, entry in enumerate(row):
                widths[i] = max(widths[i], len(entry))
        return widths

    def skip_fence():
        nonlocal pos, rows
        if pos < len(rows) and rows[pos].strip().startswith('---'):  # if we have a top fence, skip it
            pos += 1


    # Parsing starts here
    # print 'parsing Markdown table'
    skip_fence()
    headers = split_row(rows, pos)  # attempt to decode the header row
    if not headers:
        return # if unsuccessful, this is not a markdown table
    # print 'header=', headers
    pos += 1
    separators = split_row(rows, pos)
    if not separators:
        return  # bad separators = not a markdown table = give up
    # print 'separator=', separators
    pos += 1
    row_entries = []
    while pos < len(rows):
        entries = split_row(rows, pos)
        # print 'row=', entries
        if not entries:
            break
        row_entries.append(entries)
        pos += 1
    if not row_entries: return # no rows? give up, not a markdon table
    print(('We have %i header columns, %i separator columns, %s row columns' % (len(headers), len(separators), ','.join('%i' % len(r) for r in row_entries))))
    widths = max_column_widths(headers, separators, *row_entries)
    cols = len(widths)

    # print '*** widths=', widths

    table = nodes.table()
    tgroup = nodes.tgroup('', cols=cols)
    table += tgroup

    align = []
    for i, sep in enumerate(separators):
        if sep.startswith(':') and sep.endswith(':'):
            align.append('align-center')
        elif sep.startswith(':'):
            align.append('align-left')
        elif sep.endswith(':'):
            align.append('align-right')
        else:
            align.append(None)
        tgroup += nodes.colspec(colwidth=widths[i])

    thead = nodes.thead()
    tgroup += thead

    tbody = nodes.tbody()
    tgroup += tbody
    # print '*********** building table header'
    assert len(headers) == cols, 'Headers has invalid number of columns. We expected %i columns, but got got headers: %s' % (cols, ','.join(headers))
    thead += build_row(headers, align)
    for r in row_entries:
        assert len(r) == cols, 'Row has invalid number of columns'
        tbody += build_row(r, align)
    skip_fence()
    return pos, table

def parse_comment_block(state, lines, class_dict={}):
    """ Parse a block of ReStructuredText text into a node list, while also detecting and processing Markdown tables.

    Arguments:
        state: parser state to be passed to the REST parser
        lines (list of str): text to be parsed as a list of str
        class_dict (dict): classes to be applied to the REST blocks

    Returns:
        list: list of ``docutils`` nodes represting the parsed text
    """
    pos = 0
    rest_lines = []
    node_list =[]

    while pos < len(lines):
        markdown_table = parse_markdown_table(state, lines, pos)
        # print '*** Markdown table=', markdown_table
        if markdown_table:
            print('VHDL Domain: A Markdown table was detected, parsed and added')
            node_list += parse_rest(state, rest_lines, class_dict)
            rest_lines = []
            (next_pos, table_node) = markdown_table
            # print 'Markdown table nodes = ', table_node
            node_list.append(table_node)
            node_list.append(nodes.paragraph('',''))  # make sure we don't interfere with following element
            pos = next_pos
        else:
            rest_lines.append(lines[pos])
            pos += 1

    node_list += parse_rest(state, rest_lines, class_dict)
    node_list.append(nodes.paragraph('',''))  # make sure we don't interfere with following element
    return node_list


def make_vhdl_entity_table(generics, ports):
        """ Create a table that describes the ports and generics of a VHDL entity.


        Parameters:
            generics (list): List of Namespace objects describing the generics interface. Each dict
                shall contain the ``.names``, ``.definition`` and ``.comments`` attributes.

            ports (list): Same as above, but for port interfaces.

        Returns:
            docutils.nodes.table: A Docutils table containing the specified generics and ports.

        Notes:

            The table is structured as follows::

                table
                    -tgroup
                        -colspec
                        (-thead)
                        (   -row)
                        (       -entry)
                        -tbody
                            -row
                                -entry
                                -entry
                                -...
                            -row
                            -...
        """
        COLS = 2
        table = nodes.table(classes=['vhdl_entity'])
        colspec_nodes = [nodes.colspec(colwidth=0)]*(COLS-1) + [nodes.colspec(colwidth=1)]
        tgroup = nodes.tgroup('', *colspec_nodes, cols=len(colspec_nodes))
        table += tgroup
        tbody = nodes.tbody()
        tgroup += tbody

        def add_header(name, tbody=tbody):
            tbody += nodes.row('', nodes.entry('', nodes.paragraph('','', nodes.Text(name)), morecols=COLS-1), classes=['vhdl_entity_header'])

        def add_rows(interface_elements, tbody=tbody):
            """ Add a port/generic entries to `tbody` """
            for i, interface in enumerate(interface_elements):
                identifier = ','.join(interface.names)
                definition = interface.definition
                comments = interface.comments
                def_row_class = ('vhdl_entity_def_even', 'vhdl_entity_def_odd')[i & 1]
                comments_node = nodes.inline('',comments)
                # print repr(identifier), bool(identifier)
                if not identifier:  # if just a section separating comment
                    tbody += nodes.row('',
                        nodes.entry('', comments_node, morecols=1, classes=['vhdl_entity_sep']),
                        # nodes.entry(''), #
                        classes=[def_row_class])
                else: # if an actual port definition
                    identifier_node = make_lexed_vhdl_node(identifier + ':')
                    definition_node = make_lexed_vhdl_node(definition)
                    tbody += nodes.row('',
                        nodes.entry('', identifier_node, classes=['vhdl_entity_id']), #
                        nodes.entry('', definition_node, comments_node, classes=['vhdl_entity_def']), classes=[def_row_class])

        # print generics
        if generics:
            add_header('GENERICS')
            add_rows(generics)
            # print(f'Adding generics {generics}')
        if ports:
            add_header('PORTS')
            add_rows(ports)
        return table

def make_lexed_vhdl_node(text, classes = ['highlight', 'highlight-vhdl'], options={}):
    """ Create a  node that includes the specified literal text that is colorized/highlighted by Pygments for the VHDL syntax.

    Parameters:

        text (str): literal text to insert
        classes (list): list of classes to apply to the root node
        options (dict): *unused*

    Returns:
        docutils.nodes.literal: The top node containing the highlighted text nodes, such as

    """
    tokens = Lexer(unescape(text, 1), language='vhdl', tokennames='short')  # use 'short' object names so the Sphinx highlighting CSS rules will be found
    node = nodes.literal('', '', classes=classes)  # <code> element

    # analyze content and add nodes for every token
    for pygment_classes, value in tokens:
        node += nodes.inline(value, value, classes=pygment_classes)  # <span> element
    return node
